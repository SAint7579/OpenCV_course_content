{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For installing dlib \n",
    "pip install https://pypi.python.org/packages/da/06/bd3e241c4eb0a662914b3b4875fc52dd176a9db0d4a2c915ac2ad8800e9e/dlib-19.7.0-cp36-cp36m-win_amd64.whl#md5=b7330a5b2d46420343fbed5df69e6a3f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haar cascade\n",
    "\n",
    "This is one of the oldest but still a relevant object detection method which is super simple but very effective. <br>\n",
    "The logic behind is simple.....If we want to detect a face: Find the boxes where all the facial features are present.<br>\n",
    "But how to find the facial features<br>\n",
    "Thats where haar-like features come in....which are:<br>\n",
    "<img src = \"Notes_image/haar_features.jpg\">\n",
    "<br>\n",
    "These shapes can be used to estimate the features of a face (or any object for that matter) and the goal is to find a box which has all of them.<br>\n",
    "<img src = \"Notes_image/face_withhaar.jpg\">\n",
    "<br>\n",
    "Traning here is very exhaustive though. Since we are not defining the location of the object, the algorithm need to determine that on its own. This makes tranining a verrry long process..... \n",
    "<br><br><b>The steps for training are:</b>\n",
    "- Take some (actually a lot) positive and some (again...not exactly \"some\") negative images.\n",
    "- Convert all the images to grayscale.\n",
    "- Scale down the positive images to 32x32.\n",
    "- Now feed that to a training program and wait for an eternity (or get a better pc if you don't want to)....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the cascades....\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier('haarcascade_smile.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[181, 210, 222, 222]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the image\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    #Getting the frames and the grayscale\n",
    "    _, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #Get the co-ordinates for the face\n",
    "    face = face_cascade.detectMultiScale(gray, 1.3, 5) #the first one is the image, second is the scale and the last is the neighbours\n",
    "    #Face is a list of x,y,w,h of all the faces that were detected\n",
    "    for (x,y,w,h) in face:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "        #2 regions of interest for the detection and the drawing....\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "        roi_color = frame[y:y+h,x:x+w]\n",
    "\n",
    "        #Get the co-ordinates for the eyes\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray,1.5,3)\n",
    "        for (x1,y1,w1,h1) in eyes:\n",
    "            #Draw a rectangle on the eye\n",
    "            cv2.rectangle(roi_color,(x1,y1),(x1+w1,y1+h1),(0,255,0),2)\n",
    "\n",
    "        #Get the co-ordinates for that smile\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray,1.8,25)\n",
    "        for (x2,y2,w2,h2) in smiles:\n",
    "            #Now mark that smile....\n",
    "            cv2.rectangle(roi_color,(x2,y2),(x2+w2,y2+h2),(0,0,255),2)\n",
    "\n",
    "    #Now, lets look at the results.....\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Project: Drowsiness detector using DLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing the libraries first\n",
    "from scipy.spatial import distance\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets make the detection and prediction objects\n",
    "detect = dlib.get_frontal_face_detector()\n",
    "predict = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see how the detection works...\n",
    "No need to code this one out though......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    " \n",
    "while True:\n",
    "    # Getting out image by webcam \n",
    "    _, image = cap.read()\n",
    "    # Converting the image to gray scale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    # Get faces into webcam's image\n",
    "    rects = detector(gray, 0)\n",
    "    \n",
    "    # For each detected face, find the landmark.\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # Make the prediction and transfom it to numpy array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "    \n",
    "        # Draw on our image, all the finded cordinate points (x,y) \n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "    \n",
    "    # Show the image\n",
    "    cv2.imshow(\"Output\", image)\n",
    "    \n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for the goal, we will alert the driver if thier eyes are closed.\n",
    "Now, using dlib we can get 6 points on each eye. Lets use them.....\n",
    "<br>\n",
    "For the first step, we'll need the aspect ratio of the eye which will tell us whether the eyes are closed or not:\n",
    "<img src = \"Notes_image/eye3.jpg\">\n",
    "If we find the eyes closed, we can alert the driver. The closed eyes can be determined by a threshold. Lets keep it 0.25. But there is a catch...<br>\n",
    "Even when blinking, people close their eyes. So, we need to wait for some frame before alerting the driver. So lets keep that number as 10.\n",
    "<br>\n",
    "With these in mind, lets begin...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Copy this fuction....it returns the acpect ration\n",
    "def eye_aspect_ratio(eye):\n",
    "    \n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    \n",
    "    ear = (A + B)/(2.0 * C)\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now, lets get the index of those eyes.....\n",
    "(lstart, lend) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rstart, rend) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now for the frames and the aspect ration threshold.....\n",
    "frames_pass = 10\n",
    "thresh = 0.25\n",
    "#Just like we decided......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "#To keep the track of the frames.....\n",
    "flag = 0\n",
    "while True:\n",
    "    #Record the video...\n",
    "    _, frame = cam.read()\n",
    "    \n",
    "    #Now, the usual stuff\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Find the face in the grayscale image......\n",
    "    faces = detect(gray)\n",
    "    \n",
    "    for face in faces:\n",
    "        #Getting the features form the face\n",
    "        face = predict(gray, face)\n",
    "        face = face_utils.shape_to_np(face)\n",
    "        \n",
    "        #Getting the points on for the left and the right eyes\n",
    "        leftEye = face[lstart:lend]\n",
    "        rightEye = face[rstart:rend]\n",
    "        \n",
    "        #Calculating the aspect ration for both the eyes\n",
    "        leftEar = eye_aspect_ratio(leftEye)\n",
    "        rightEar = eye_aspect_ratio(rightEye)\n",
    "        \n",
    "        #Finding the net aspect ration.....\n",
    "        ear = (leftEar + rightEar)/2.0\n",
    "        \n",
    "        #Lets draw the convex hull for the sake of it.....it'll make it look cool....\n",
    "        left_conv = cv2.convexHull(leftEye)\n",
    "        right_conv = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [left_conv], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [right_conv], -1, (0, 255, 0), 1)\n",
    "        \n",
    "        #Now for the drowsyness detector part.....\n",
    "        if ear < thresh:\n",
    "            flag += 1\n",
    "            \n",
    "            if flag > frames_pass:\n",
    "                #Alert the user if he is sleepy....\n",
    "                \n",
    "                cv2.putText(frame, \"********ALERT*********\",\n",
    "                           (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                cv2.putText(frame, \"YOU ARE DROWSY\",\n",
    "                           (10, 324), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "            else:\n",
    "                cv2.putText(frame, \"AWAKE!!!\",\n",
    "                           (10, 324), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        \n",
    "        else:\n",
    "            #Reset the counted number of frames\n",
    "            flag = 0 \n",
    "    \n",
    "    cv2.imshow(\"Live\", frame)\n",
    "   \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
